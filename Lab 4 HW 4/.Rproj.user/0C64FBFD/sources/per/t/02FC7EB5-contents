---
title: "PSTAT 5LS Lab 4" 
author: "TA Name Here"
date: "Week of May 6, 2024"
output:
  beamer_presentation:
    theme: AnnArbor
    fig_crop: false
    fig_width: 6
    fig_height: 4
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(stats250sbi)
```

# Announcements & Recap

## Announcements
Insert any relevant announcements, important dates, things you want to remember here.

You might want to tell the students about any of the following:

- Exam 1 available in Gradescope. Regrade requests available through 8am on Monday, May 13
- ECoach Journal Entry 5 due by 11:59pm on Monday, May 6
- HW 4 due by 11:59pm on Friday, May 10
- ECoach Journal Entry 6 due by 11:59pm on Monday, May 13

# Learning Objectives

## R Learning Objectives
1. Revisit simulation for one proportion
1. Revisit `pnorm()` and `qnorm()`
1. Introduce `prop_test()` to understand test statistics and confidence intervals

## Statistical Learning Objectives
1. Understand how area under the normal curve relates to probability
1. Understand how to move between probabilities and quantiles of the normal distribution
1. Build intuition for the relationship between simulated p-values and p-values which arise from a normal approximation.
1. Understand the standard normal distribution and its corresponding $z$ test statistic.
1. Understand how confidence intervals can provide an estimate for the true parameter.

## Functions covered in this lab
1. `plot_norm()`
1. `pnorm()`
1. `qnorm()`
1. `prop_test()`

# Lab Tutorial

## Another Simulation Example

According to the American Pet Products Association, prior to the pandemic, 67% of all U.S. households had pets. Many people have speculated that the rate of pet ownership changed during the pandemic. Recently, a group of veterinarians surveyed a random sample of 480 U.S. households and found that 336 had at least one pet.

Is there evidence to support the claim that the rate of pet ownership differs from the 67% before the pandemic?

Our hypotheses to test if there is a difference are 
$$H_0: p = 0.67 \text{ and } H_A: p \neq 0.67$$

## Setting Up the Simulation

|                   | **Assuming the chance model...** |
|-------------------|----------------------------------|
| One draw          | *one household* |
| Blue poker chip   | *the household has at least one pet* |
| Yellow poker chip | *the household has no pets* |
| Chance of blue    | *0.67* |
| One repetition    | *480 households* |

## Running the Simulation

Recall that we need to set a seed before running a simulation. Let's arbitrarily set the seed to 734. Run this in the `setSeed` code chunk.

```{r setSeed, echo = FALSE}
set.seed(734)
```

Then let's run the simulation 500 times. Run this in the `sim1` code chunk.

```{r sim1}
sim1 <- simulate_chance_model(chanceSuccess = 0.67, 
                              numDraws = 480, 
                              numRepetitions = 500)
```

`sim1` should now be in your Environment pane.

## Histogram of Simulation Results

The sample statistic is $\hat p = \frac{336}{480} = 0.70$, so we will put a line in the histogram to indicate our observed statistic. 

In our two-sided hypothesis test, it would have been just as unusual to see a sample proportion of 0.64, so we will put a line in the histogram to indicate this as well.

## Histogram of Simulation Results

```{r histSimulate, echo = FALSE}
hist(sim1, 
     main = "Histogram of 500 Simulation Results",
     xlab = "Simulated proportion of U.S households with at least one pet")
abline(v = 0.70, col = "red")
abline(v = 0.64, col = "red")
```

Run the `histSimulate` code chunk to duplicate this histogram.

## Estimated p-value
The estimated p-value is the sum of the simulations that are as weird as or weirder than we observed (so, 0.70 or larger) *and* the sum of the simulations that are as weird or weirder on the other side (at 0.64 or less). Run this in the `pvalue_sim1` code chunk.

```{r pvalue_sim1, echo = FALSE}
sum(sim1 >= 0.70 | sim1 <= 0.64) / 500
```

Our estimated p-value is 0.156.

## Conclusion
After running the simulation 500 times, assuming that the chance of success was indeed 0.67, the probability that we would get an observed sample proportion of 336/480 or more extreme was computed to be 0.156.

The p-value of 0.156 is larger than even a significance level of 0.10. We have little to no evidence to support the claim that the rate of pet ownership differs from the 67% before the pandemic.

## But We Observed a Different Result?!
You might have asked yourself, "But wait, why do we have little to no evidence to support the claim? I did in fact observe a rate different than 67% in my sample (I observed 70%, in fact). Isn't a 3% difference strong evidence?"

This is where the normal theory can help us understand that the difference of 3% between the $\hat{p}$ and the assumed $p$ was not enough evidence.

## Reminder about Normal Distributions
Recall that a "distribution" refers to the possible values a random variable can take as well as the probability that it takes those values. It is commonly used to approximate all sorts of things in nature and life.

It takes two numbers (parameters) to completely describe a normal distribution: the **mean** and **standard deviation**. We denote a normal distribution by N($\mu$, $\sigma$), where $\mu$ is the population mean and $\sigma$ is the population standard deviation. 

Normal distributions are all bell-shaped, unimodal, and symmetric about their means, regardless of the values of the mean ($\mu$) and the standard deviation ($\sigma$). 

The mean $\mu$ specifies the *center* of the distribution. The standard deviation $\sigma$ specifies the *variability* of the distribution (meaning, how narrow or wide is it?).

## Histogram from Simulation
It turns out, the histogram that we created using simulation is in fact approximately normal. 

We talked about how the distribution is centered at $H_0$, the chance of success. Thus we can assume $\mu = 0.67$ in our example about pet ownership prior to the pandemic.

To find the standard error, which is an estimate of the standard deviation for our sample statistic $\hat p$, we will use the following formula:

$$\sqrt{\frac{p_0(1-p_0)}{n}}$$
For the example about pet ownership, let's compute the standard error, $SE(\hat{p})$.

## Histogram from Simulation
$$\sqrt{\frac{p_0(1-p_0)}{n}} = \sqrt{\frac{0.67(1-0.67)}{480}} = 0.021$$
Run the `computeSE` chunk in your notes to calculate the SE.

Thus, the sampling distribution of $\hat p$ in the pet ownership example will be approximately N(0.67, 0.021).

## Normal Distribution for Our Example

Let's take a look at the approximate normal distribution for our pet ownership example. Recall that, when the null hypothesis is true, the mean is **0.67**, the standard error is **0.021**, and we want to shade **beyond** the values of 0.64 and 0.7. Run the `normalDistCode` chunk in your notes.

```{r normalDistCode, eval = FALSE}
plot_norm(mean = 0.67, 
          sd = 0.021, 
          shadeValues = c(0.64, 0.7), 
          direction = "beyond", 
          col.shade = "darkgreen")
```

## Normal Distribution for Example
```{r normalDistGraph, echo = FALSE}
plot_norm(mean = 0.67, 
          sd = 0.021, 
          shadeValues = c(0.64, 0.7), 
          direction = "beyond", 
          col.shade = "darkgreen")
```

## There Is in Fact Insufficient Evidence
Now that we see that the standard deviation is 0.021, or 2.1%, we can start to understand why observing a rate that was only 3% different from the assumed mean is not all that rare. 

Looking at the normal distribution plot, we can observe that 0.7 is only between 1 and 2 standard deviations above the mean. Again, not all that rare.

## Another Way to Calculate the p-value
The `stats250sbi` package we have been using also contains a function called `prop_test()` that will calculate the value of the test statistic and the p-value for us. 

You will need to send the function the following arguments:

- `x`: the number of "successes" in the sample
- `n`: the sample size
- `p`: the hypothesized population proportion
- `alternative`: where to shade (`"two.sided"`, `"less"`, `"greater"`)
- `conf.level` (optional): used to get a confidence interval (we will use this later)

## Using `prop_test` for Our Pet Ownership Example

The code we need for the test of $H_0: p = 0.67 \text{ and } H_A: p \neq 0.67$ is

```{r prop_test_pet, eval = FALSE}
prop_test(x = 336, 
          n = 480, 
          p = 0.67, 
          alternative = "two.sided")
```

## Using `prop_test` for Our Pet Ownership Example


```{r prop_test_pet1, echo = FALSE}
prop_test(x = 336, 
          n = 480, 
          p = 0.67, 
          alternative = "two.sided")
```

Run the `prop_test_pet` code chunk in your notes to match this output.

## The `pnorm()` Function
The p-value from `prop_test()` is pretty close to the p-value from our simulation. 

We can also compute the p-value using the `pnorm()` function. Recall the arguments you need to send to `pnorm()`:

- `q`: the quantile (value on the axis) for the normal distribution
- `mean`: the mean of the normal distribution ($\mu$)
- `sd`: the standard deviation of the normal distribution ($\sigma$)
- `lower.tail`: set to \textcolor{blue}{`TRUE`} initially, signifying that R will compute the probability **to the \textcolor{blue}{LEFT}** of `q`; if you would like R to compute the probability *to the \textcolor{red}{right}* of `q`, set `lower.tail` to \textcolor{red}{FALSE}

## Computing the p-value for the Simulation Example
Let's compute the approximate p-value using the normal distribution for our pet ownership example. Recall that the mean is **0.67**, and the standard deviation is **0.021**.

Because the normal distribution is **symmetric** about the mean, we can find the probability of observing 0.7 or greater, then **double it**. 

Or, we can find the probability of observing 0.64 or less, then **double it**.

Or, we can find the probability of observing 0.70 or greater, then the probability of observing 0.64 or less, and add the result.

Since 0.7 is the $\hat{p}$ for our sample, we will utilize the first option in the `pvalue` code chunk.

## Computing the p-value for the Simulation Example

Run this code in the `pvalue` code chunk.

```{r pvalue}
2 * pnorm(q = 0.7, 
      mean = 0.67, 
      sd = 0.021, 
      lower.tail = FALSE)
```

## Comparing the Various p-values
To recap, we have *three* ways to compute the p-value for a one proportion hypothesis test:

1. Create a vector of simulated proportions using `simulate_chance_model()`, then using the `sum()` function to count the number of observations at or beyond the sample proportion divided by the number of observations 
1. Use the `prop_test()` function (which uses normal theory) by sending the number of successes observed, the sample size, the value of $H_0$, and the direction of the alternative hypothesis 
1. Compute the $\mu$ and $\sigma$ for the approximate normal distribution. Use the `pnorm()` function by sending the value of the sample proportion, $\mu$, $\sigma$, and the direction of the probability 

Each of these will produce a slightly different result. **No need to worry about how close the values should be, or which value is "best".**

## Comparing the Various p-values
\scriptsize
```{r comparepValue}
sum(sim1 <= 0.64) / 500 + sum(sim1 >= 0.7) / 500
prop_test(x = 336, n = 480, p = 0.67)
pnorm(q = 0.7, mean = 0.67, sd = 0.021, lower.tail = FALSE) * 2
```
\normalsize

## Comparing the Various p-values
1. The simulation is the most accurate, because it is computing the p-value with simulated values.
1. `prop_test()` and `pnorm()` will lose some precision due to utilizing the normal approximation. This loss of precision should not affect our results.
1. `pnorm()` will lose some precision if we round the standard deviation to 3 decimal places. This loss of precision should not affect our results.

## Using `prop_test()` to Find Confidence Intervals
The output from `prop_test()` also provides a confidence interval for the population proportion. The default confidence level is 0.95 for a 95% confidence interval. The 95% confidence interval for this example is (0.659, 0.741).

```{r prop_test_pet2, echo = FALSE}
prop_test(x = 336, 
          n = 480, 
          p = 0.67, 
          alternative = "two.sided")
```

We didn't need to specify that we wanted a 95% confidence interval because that is the default.

## Using `prop_test()` to Find Confidence Intervals
We can change the confidence level to a level other than 95% by adding the argument `conf.level` to the `prop_test()` function. For example, a 98% confidence interval is (0.651, 0.749).

```{r prop_test_pet3, echo = FALSE}
prop_test(x = 336, 
          n = 480, 
          p = 0.67, 
          alternative = "two.sided", 
          conf.level = 0.98)
```

## Using `prop_test()` to Find Confidence Intervals
**Caution:** To get a two-sided confidence interval, the `alternative` argument *must* be set to `two.sided`. If it isn't, you will get a *confidence bound*. 

Confidence bounds can be useful when we have one-sided hypothesis tests, but we will leave them to your later statistics courses. 

## One More Example

X, the social media platform formerly known as Twitter, saw a name change and policy changes once it was under new ownership. Shortly after the name change, seventy-five percent (75%) of all X users said that they planned to keep using X in 2024. Researchers at X would like to know if this rate differs for Millennials, those born between 1981 and 1996. A random sample of 100 Millennial users of X revealed that 65% planned to keep using X in 2024.

## What Would We Expect to See in the Sample?

If the rate of current Millennial users of X is the same as the rate for all X users, how many Millennial users of X from the sample of 100 would we expect to say that they planned to keep using X in 2024?

## Incorrect Alternative

\small
The researchers at X wanted to know if the rate of Millennials who planned to continue using X in 2024 differs from the rate of all users of X. The researcher in charge accidentally ran a one-sided hypothesis test and got the following output

```{r incorrect_alternative}
prop_test(x = 65, n = 100, p = 0.75, alternative = "less")
```

## Correcting the Mistake

The value of the test statistic for the incorrect alternative was $z = -2.309$. What should the test statistic be for the correct two-sided test?

- It should be half as much; that is, $z = -1.155$.
- It should be positive instead of negative; that is, $z = 2.309$.
- It should be the same; that is, $z = -2.309$.
- It should be twice as much; that is, $z = -4.618$.

## Correcting the Mistake

The p-value for the incorrect alternative was 0.01046. What should the p-value be for the correct two-sided test?

- It should be half as much; that is, p-value = 0.00523.
- It should be negative instead of positive; that is, p-value = $-0.01046$.
- It should be the same; that is, p-value = 0.01046.
- It should be twice as much; that is, p-value = 0.02092.


# Questions

## What Questions Do You Have?
